name: Fix Scrapers

on:
  repository_dispatch:
    types: [fix-scrapers]
  workflow_dispatch:
    inputs:
      run_id:
        description: 'Run ID of the failed scrape workflow (optional - will use latest errors)'
        required: false
        type: string
      dry_run:
        description: 'Dry run - do not create PR'
        required: false
        default: false
        type: boolean

jobs:
  fix:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium

      - name: Download scraper errors artifact
        if: github.event.client_payload.run_id || inputs.run_id
        uses: actions/download-artifact@v4
        with:
          name: scraper-errors
          path: artifacts
          run-id: ${{ github.event.client_payload.run_id || inputs.run_id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true

      - name: Check for errors file
        id: check-errors
        run: |
          if [ -f "artifacts/scraper_errors.json" ]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
            echo "Errors found:"
            cat artifacts/scraper_errors.json
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
            echo "No errors file found"
          fi

      - name: Install Amp CLI
        if: steps.check-errors.outputs.has_errors == 'true'
        run: npm install -g @anthropic-ai/amp

      - name: Run Amp to fix scrapers
        if: steps.check-errors.outputs.has_errors == 'true'
        env:
          AMP_API_KEY: ${{ secrets.AMP_API_KEY }}
        run: |
          # Extract first error from the errors file
          SCRAPER_NAME=$(python3 -c "import json; d=json.load(open('artifacts/scraper_errors.json')); print(d['errors'][0]['scraper_name'])")
          CATEGORY=$(python3 -c "import json; d=json.load(open('artifacts/scraper_errors.json')); print(d['errors'][0]['category'] or 'unknown')")
          ERROR_MSG=$(python3 -c "import json; d=json.load(open('artifacts/scraper_errors.json')); print(d['errors'][0]['error_message'])")
          EVENTS_URL=$(python3 -c "import json; d=json.load(open('artifacts/scraper_errors.json')); print(d['errors'][0]['events_url'] or 'unknown')")
          
          echo "Fixing scraper: $SCRAPER_NAME ($CATEGORY)"
          echo "Error: $ERROR_MSG"
          echo "URL: $EVENTS_URL"
          
          # Create the prompt for Amp
          PROMPT="You are fixing a broken scraper for the CulturÄƒ la plic event aggregator.

          SCRAPER: $SCRAPER_NAME
          CATEGORY: $CATEGORY
          FILE: scrapers/$CATEGORY/$SCRAPER_NAME.py
          ERROR: $ERROR_MSG
          EVENTS_URL: $EVENTS_URL

          INSTRUCTIONS:
          1. First, load the generating-scrapers skill using the skill tool
          2. Read the current scraper code to understand the existing selectors
          3. Create tmp/ directory if needed: mkdir -p tmp
          
          4. VISUAL INSPECTION (repeat as needed):
             Take a screenshot and inspect the page structure using Playwright:
             python3 -c \"
             from playwright.sync_api import sync_playwright
             with sync_playwright() as p:
                 browser = p.chromium.launch()
                 page = browser.new_page()
                 page.goto('$EVENTS_URL')
                 page.wait_for_timeout(3000)
                 page.screenshot(path='tmp/page_screenshot.png', full_page=True)
                 # Get accessibility tree for structure
                 snapshot = page.accessibility.snapshot()
                 print('Page title:', page.title())
                 print('Accessibility tree:', snapshot)
                 browser.close()
             \"
             Then use the look_at tool to examine tmp/page_screenshot.png and understand the visual layout.
          
          5. Get the raw HTML to find correct selectors:
             python3 -c \"
             from playwright.sync_api import sync_playwright
             with sync_playwright() as p:
                 browser = p.chromium.launch()
                 page = browser.new_page()
                 page.goto('$EVENTS_URL')
                 page.wait_for_timeout(3000)
                 print(page.content())
                 browser.close()
             \" > tmp/page_content.html
             Then read tmp/page_content.html to find the correct CSS selectors.
          
          6. Fix the scraper code based on what you found
          
          7. TEST THE FIX:
             python3 -c \"from scrapers.$CATEGORY.$SCRAPER_NAME import scrape; events = scrape(); print(f'{len(events)} events found'); [print(f'  - {e.title}') for e in events[:3]]\"
          
          8. ITERATE: If 0 events returned:
             - Take another screenshot to see current page state
             - Check if page needs JS rendering (needs_js=True)
             - Check if selectors need adjustment
             - Fix and test again
             - Repeat until events are returned
          
          9. Once working (events > 0), confirm the fix is complete

          Do NOT commit any changes. Just fix the code and verify it works."

          # Run Amp with the prompt (execute mode, allow all tools)
          echo "$PROMPT" | amp --execute --dangerously-allow-all

      - name: Create fix branch and PR
        if: steps.check-errors.outputs.has_errors == 'true' && inputs.dry_run != true
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          SCRAPER_NAME=$(python3 -c "import json; d=json.load(open('artifacts/scraper_errors.json')); print(d['errors'][0]['scraper_name'])")
          CATEGORY=$(python3 -c "import json; d=json.load(open('artifacts/scraper_errors.json')); print(d['errors'][0]['category'] or 'unknown')")
          BRANCH_NAME="bot/fix-$SCRAPER_NAME-$(date +%Y%m%d)"
          
          # Check if there are any changes
          if git diff --quiet; then
            echo "No changes detected - scraper may not have been fixed"
            exit 0
          fi
          
          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Create branch and commit
          git checkout -b "$BRANCH_NAME"
          git add "scrapers/$CATEGORY/$SCRAPER_NAME.py" "services/enrichment.py" || true
          git commit -m "Fix $SCRAPER_NAME scraper (auto-generated)"
          git push -u origin "$BRANCH_NAME"
          
          # Create PR
          gh pr create \
            --title "Fix $SCRAPER_NAME scraper (auto-generated)" \
            --body "This PR was automatically generated by the fix-scrapers workflow.

          **Scraper:** \`$SCRAPER_NAME\`
          **Category:** \`$CATEGORY\`

          Please review the changes carefully before merging.

          ---
          *Generated by fix-scrapers workflow*" \
            --label "auto-fix"
